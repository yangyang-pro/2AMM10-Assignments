{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v_3Kdq4tqYC"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment1/workbook-task1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Modify this cell to add your group name, group number and your names and student IDs\n",
    "\n",
    "Group: 99\n",
    "\n",
    "Authors: Yang Yang, Ying Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuVJoCROtogK",
    "outputId": "8e781f12-1bce-4112-d826-8d07868dea97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x0WIjQ7togW"
   },
   "source": [
    "### Training data set\n",
    "\n",
    "For task 1 of Assignment 1 you need to use a specific data set prepared using images from the [Omniglot dataset](https://github.com/brendenlake/omniglot). The provided training data set contains 18.800 binary images of handwritten characters of size (28,28). Each of these images depicts one of 893 different characters from 29 different alphabets. Each image is accompanied by a label that is encoded as an interger $y\\in\\{0, 1, ..., 892\\}$ that indicate the caracter depicted in the image. The following cell provides code that loads the data from hardcoded URLs.\n",
    "\n",
    "You can use the code in this cell to load the dataset or download the data set from the given URLs to your local drive (or your Google drive) and modify the code to load the data from another location. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLCixBNZtogX",
    "outputId": "a1f77d5c-ac1d-4f57-c2f7-e8f411ed9eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (18800, 28, 28)\n",
      "train_y shape: (18800,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_numpy_arr_from_url(url):\n",
    "    \"\"\"\n",
    "    Loads a numpy array from surfdrive. \n",
    "    \n",
    "    Input:\n",
    "    url: Download link of dataset \n",
    "    \n",
    "    Outputs:\n",
    "    dataset: numpy array with input features or labels\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return np.load(io.BytesIO(response.content)) \n",
    "    \n",
    "    \n",
    "    \n",
    "# Downloading may take a while..\n",
    "train_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/tvQmLyY7MhVsADb/download')\n",
    "train_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/z234AHrQqx9RVGH/download')\n",
    "\n",
    "print(f\"train_x shape: {train_x.shape}\")\n",
    "print(f\"train_y shape: {train_y.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# train validation split\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, stratify=train_y)\n",
    "train_x, val_x = np.expand_dims(train_x, axis=1).astype(float), np.expand_dims(val_x, axis=1).astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsyaadG4togZ"
   },
   "source": [
    "### Query data set\n",
    "\n",
    "For this task you need to use the following query data set. The dataset contains 100 sets of 6 images each. The images are also of hand written characters, however these characters are not present in the training data set. The characters in the query data set all come from the Greek alphabet that is not part of the set of alphabets in the training data. \n",
    "\n",
    "Each test set consists of 1 query image and 5 candidate images. All images are the same size (28x28). The test data is organized in two numpy arrays. One for the query images with shape (100, 1, 28, 28) and another for the candidate imagaes with shape (100, 5, 28, 28). \n",
    "\n",
    "The task is to develop a model that enables selecting the image which is depicting the same character as the anchor image out of 5 test images. These test images are declared in the `query_x` numpy array . \n",
    "\n",
    "Finally, we plot the first 5 cases in the query dataset. The first column corresponds with the anchor images of each of the 5 cases. All other images are test images from which the task is to recognize the anchor image. The image enclosed in a red box denotes the target image that your model should be able to recognize as the same class as the anchor image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHZcmBr4togZ",
    "outputId": "676ff913-f0f0-4b07-acce-37d79872fcf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query images have shape: (100, 1, 28, 28)\n",
      "target sets have shape: (100, 5, 28, 28)\n",
      "ground truth: (100,)\n"
     ]
    }
   ],
   "source": [
    "query_dataset = load_numpy_arr_from_url(\"https://surfdrive.surf.nl/files/index.php/s/YGn5gb7unBEuCLB/download\")\n",
    "queries_true = load_numpy_arr_from_url(\"https://surfdrive.surf.nl/files/index.php/s/0sPeeIFB3W9RPZG/download\")\n",
    "\n",
    "queries, candidates_sets = np.split(query_dataset, [1], axis=1)\n",
    "\n",
    "print(f\"query images have shape: {queries.shape}\")\n",
    "print(f\"target sets have shape: {candidates_sets.shape}\")\n",
    "print(f\"ground truth: {queries_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def plot_case(caseID):\n",
    "    \"\"\"\n",
    "    Plots a single sample of the query dataset\n",
    "    \n",
    "    Inputs\n",
    "    caseID: Integer between 0 and 99, each corresponding to a single sample in the query dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    f, axes = plt.subplots(1, 6, figsize=(20,5))\n",
    "    \n",
    "    # plot anchor image\n",
    "    axes[0].imshow(queries[caseID, 0])\n",
    "    axes[0].set_title(f\"Anchor image case {caseID}\", fontsize=10)\n",
    "    \n",
    "    # show all test images images \n",
    "    [ax.imshow(candidates_sets[caseID, i]) for i, ax in enumerate(axes[1:])]\n",
    "    \n",
    "    \n",
    "    # Add the patch to the Axes\n",
    "    axes[queries_true[caseID]].add_patch(Rectangle((0,0),27,27,linewidth=2, edgecolor='r',facecolor='none'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# plot the first five samples of the query datset\n",
    "[plot_case(caseID) for caseID in range(5)] ;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ObMV6qoI0vc9"
   },
   "outputs": [],
   "source": [
    "##\n",
    "# TODO: Implement your recognition model here...\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        # conv layers\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels))\n",
    "        self.label_to_indices = {label: np.where(  np.array(self.labels) == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class RandomTripletSelector():\n",
    "    \"\"\"\n",
    "    Select random negative  example for  each positive pair  to create triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RandomTripletSelector, self).__init__()\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "\n",
    "            # random choose one negative example for each positive pair\n",
    "            temp_triplets = [[anchor_positive[0], anchor_positive[1], np.random.choice(negative_indices)] for anchor_positive in anchor_positives]\n",
    "            triplets += temp_triplets\n",
    "\n",
    "        return torch.LongTensor(np.array(triplets))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def pdist(vectors):\n",
    "    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n",
    "        dim=1).view(-1, 1)\n",
    "    return distance_matrix\n",
    "\n",
    "class Informative_Negative_TripletSelector():\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(Informative_Negative_TripletSelector, self).__init__()\n",
    "\n",
    "        self.margin = margin\n",
    "\n",
    "   # Our goal is to mining informative triplets.\n",
    "    def informative_negative(self, loss_values):\n",
    "\n",
    "        informative_negative = np.where(loss_values > 0)[0]\n",
    "        return np.random.choice(informative_negative) if len(informative_negative) > 0 else None\n",
    "\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "\n",
    "        if torch.cuda.is_available()==False:\n",
    "            embeddings = embeddings.cpu()\n",
    "        distance_matrix = pdist(embeddings)\n",
    "        distance_matrix = distance_matrix.cpu()\n",
    "\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "            anchor_positives = np.array(anchor_positives)\n",
    "\n",
    "            #randomly choose one negative example in the mined informative examples (semihard or hard negative samples) such that the triplets formed is semihard or hard triplet.\n",
    "\n",
    "            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n",
    "            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n",
    "                loss_values = ap_distance - distance_matrix[torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n",
    "                loss_values = loss_values.data.cpu().numpy()\n",
    "\n",
    "                hard_negative = self.informative_negative(loss_values)\n",
    "                if hard_negative is not None:\n",
    "                    hard_negative = negative_indices[hard_negative]\n",
    "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
    "\n",
    "            if len(triplets) == 0:\n",
    "                triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n",
    "\n",
    "        triplets = np.array(triplets)\n",
    "\n",
    "        return torch.LongTensor(triplets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
    "    triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, triplet_selector):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.triplet_selector = triplet_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "\n",
    "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            triplets = triplets.cuda()\n",
    "\n",
    "\n",
    "        anchor_idx= triplets[:, 0]\n",
    "        positive_idx= triplets[:, 1]\n",
    "        negative_idx= triplets[:, 2]\n",
    "\n",
    "\n",
    "        ap_distances = (embeddings[anchor_idx] - embeddings[positive_idx]).pow(2).sum(1)  # .pow(.5)\n",
    "        an_distances = (embeddings[anchor_idx] - embeddings[negative_idx]).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_dataLoader: torch.utils.data.Dataset,\n",
    "                 validation_dataLoader: torch.utils.data.Dataset ,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_dataLoader = training_dataLoader\n",
    "        self.validation_dataLoader = validation_dataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def run_trainer(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            self.model.train()  # train mode\n",
    "            train_losses=[]\n",
    "            for batch in self.training_dataLoader:\n",
    "                x, y = batch\n",
    "                input, target = x.to(device=self.device, dtype=torch.float), y.to(self.device) # send to device (GPU or CPU)\n",
    "                self.optimizer.zero_grad()  # zerograd the parameters\n",
    "                out = self.model(input)  # one forward pass\n",
    "                loss = self.criterion(out, target)  # calculate training loss\n",
    "\n",
    "                loss_value = loss.item()\n",
    "                train_losses.append(loss_value)\n",
    "\n",
    "                loss.backward()  # one backward pass\n",
    "                self.optimizer.step()  # update the parameters\n",
    "\n",
    "            self.model.eval()  # evaluation mode\n",
    "            val_losses = []  # accumulate the losses here\n",
    "\n",
    "            for batch in self.validation_dataLoader:\n",
    "                x, y = batch\n",
    "                input, target = x.to(device=self.device, dtype=torch.float), y.to(device=self.device)  # send to device (GPU or CPU)\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(input)   # one forward pass\n",
    "                    loss = self.criterion(out, target) # calculate validation loss\n",
    "\n",
    "                    loss_value = loss.item()\n",
    "                    val_losses.append(loss_value)\n",
    "            print('Epoch:', epoch)\n",
    "            print('Training loss,', np.mean(train_losses))\n",
    "            print('Validation loss,', np.mean(val_losses))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_x), torch.tensor(train_y))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(val_x), torch.tensor(val_y))\n",
    "\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.tensors[1].cpu().data.numpy(), n_classes=100, n_samples=10)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.tensors[1].cpu().data.numpy(), n_classes=100, n_samples=10)\n",
    "\n",
    "triplets_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler)\n",
    "triplets_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:11<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training loss, 0.9792059250175953\n",
      "Validation loss, 0.977503776550293\n",
      "Epoch: 1\n",
      "Training loss, 0.9704671986401081\n",
      "Validation loss, 0.9551751613616943\n",
      "Epoch: 2\n",
      "Training loss, 0.958267442882061\n",
      "Validation loss, 0.9546046257019043\n",
      "Epoch: 3\n",
      "Training loss, 0.9344627372920513\n",
      "Validation loss, 0.884003221988678\n",
      "Epoch: 4\n",
      "Training loss, 0.8746320568025112\n",
      "Validation loss, 0.79997318983078\n",
      "Epoch: 5\n",
      "Training loss, 0.7416646853089333\n",
      "Validation loss, 0.6236447691917419\n",
      "Epoch: 6\n",
      "Training loss, 0.5893046874552965\n",
      "Validation loss, 0.4316042959690094\n",
      "Epoch: 7\n",
      "Training loss, 0.5096978936344385\n",
      "Validation loss, 0.5794863104820251\n",
      "Epoch: 8\n",
      "Training loss, 0.4748750012367964\n",
      "Validation loss, 0.642109215259552\n",
      "Epoch: 9\n",
      "Training loss, 0.43122006207704544\n",
      "Validation loss, 0.5654462575912476\n",
      "Epoch: 10\n",
      "Training loss, 0.42223518155515194\n",
      "Validation loss, 0.4115157127380371\n",
      "Epoch: 11\n",
      "Training loss, 0.3870532885193825\n",
      "Validation loss, 0.42639318108558655\n",
      "Epoch: 12\n",
      "Training loss, 0.36979578249156475\n",
      "Validation loss, 0.3677171766757965\n",
      "Epoch: 13\n",
      "Training loss, 0.34158633276820183\n",
      "Validation loss, 0.3922095000743866\n",
      "Epoch: 14\n",
      "Training loss, 0.3196314573287964\n",
      "Validation loss, 0.32643923163414\n",
      "Epoch: 15\n",
      "Training loss, 0.3055434487760067\n",
      "Validation loss, 0.29013320803642273\n",
      "Epoch: 16\n",
      "Training loss, 0.3071096818894148\n",
      "Validation loss, 0.3336319923400879\n",
      "Epoch: 17\n",
      "Training loss, 0.29481955617666245\n",
      "Validation loss, 0.19757680594921112\n",
      "Epoch: 18\n",
      "Training loss, 0.2737692007794976\n",
      "Validation loss, 0.30824145674705505\n",
      "Epoch: 19\n",
      "Training loss, 0.26918421033769846\n",
      "Validation loss, 0.2942458391189575\n",
      "Epoch: 20\n",
      "Training loss, 0.26809602696448565\n",
      "Validation loss, 0.263929545879364\n",
      "Epoch: 21\n",
      "Training loss, 0.2540759015828371\n",
      "Validation loss, 0.27431222796440125\n",
      "Epoch: 22\n",
      "Training loss, 0.2506198175251484\n",
      "Validation loss, 0.281769335269928\n",
      "Epoch: 23\n",
      "Training loss, 0.24761936720460653\n",
      "Validation loss, 0.3276311457157135\n",
      "Epoch: 24\n",
      "Training loss, 0.23628832958638668\n",
      "Validation loss, 0.20406129956245422\n",
      "Epoch: 25\n",
      "Training loss, 0.24059527181088924\n",
      "Validation loss, 0.2613493800163269\n",
      "Epoch: 26\n",
      "Training loss, 0.23848402965813875\n",
      "Validation loss, 0.1989070028066635\n",
      "Epoch: 27\n",
      "Training loss, 0.2228390797972679\n",
      "Validation loss, 0.2183915674686432\n",
      "Epoch: 28\n",
      "Training loss, 0.22388210333883762\n",
      "Validation loss, 0.20603640377521515\n",
      "Epoch: 29\n",
      "Training loss, 0.21904281433671713\n",
      "Validation loss, 0.24931514263153076\n",
      "Epoch: 30\n",
      "Training loss, 0.21575598791241646\n",
      "Validation loss, 0.22796140611171722\n",
      "Epoch: 31\n",
      "Training loss, 0.1962794065475464\n",
      "Validation loss, 0.22817343473434448\n",
      "Epoch: 32\n",
      "Training loss, 0.20539437048137188\n",
      "Validation loss, 0.27174022793769836\n",
      "Epoch: 33\n",
      "Training loss, 0.19817555788904428\n",
      "Validation loss, 0.1463085263967514\n",
      "Epoch: 34\n",
      "Training loss, 0.20182684436440468\n",
      "Validation loss, 0.20744800567626953\n",
      "Epoch: 35\n",
      "Training loss, 0.19811529200524092\n",
      "Validation loss, 0.14244720339775085\n",
      "Epoch: 36\n",
      "Training loss, 0.19628860335797071\n",
      "Validation loss, 0.2237277328968048\n",
      "Epoch: 37\n",
      "Training loss, 0.19825639389455318\n",
      "Validation loss, 0.22429177165031433\n",
      "Epoch: 38\n",
      "Training loss, 0.18918593507260084\n",
      "Validation loss, 0.20780782401561737\n",
      "Epoch: 39\n",
      "Training loss, 0.17938126903027296\n",
      "Validation loss, 0.1782141774892807\n",
      "Epoch: 40\n",
      "Training loss, 0.17916360683739185\n",
      "Validation loss, 0.2606506049633026\n",
      "Epoch: 41\n",
      "Training loss, 0.1740730870515108\n",
      "Validation loss, 0.15947271883487701\n",
      "Epoch: 42\n",
      "Training loss, 0.18149563018232584\n",
      "Validation loss, 0.13118940591812134\n",
      "Epoch: 43\n",
      "Training loss, 0.17612868081778288\n",
      "Validation loss, 0.2042350322008133\n",
      "Epoch: 44\n",
      "Training loss, 0.16780670918524265\n",
      "Validation loss, 0.11547231674194336\n",
      "Epoch: 45\n",
      "Training loss, 0.17219091206789017\n",
      "Validation loss, 0.18686828017234802\n",
      "Epoch: 46\n",
      "Training loss, 0.1766512207686901\n",
      "Validation loss, 0.15923283994197845\n",
      "Epoch: 47\n",
      "Training loss, 0.16766634956002235\n",
      "Validation loss, 0.15250112116336823\n",
      "Epoch: 48\n",
      "Training loss, 0.16434174682945013\n",
      "Validation loss, 0.10741551965475082\n",
      "Epoch: 49\n",
      "Training loss, 0.1606415966525674\n",
      "Validation loss, 0.20323973894119263\n",
      "Epoch: 50\n",
      "Training loss, 0.16098541300743818\n",
      "Validation loss, 0.20610305666923523\n",
      "Epoch: 51\n",
      "Training loss, 0.1610576929524541\n",
      "Validation loss, 0.1660735309123993\n",
      "Epoch: 52\n",
      "Training loss, 0.15879026986658573\n",
      "Validation loss, 0.21532393991947174\n",
      "Epoch: 53\n",
      "Training loss, 0.15110927913337946\n",
      "Validation loss, 0.0722360908985138\n",
      "Epoch: 54\n",
      "Training loss, 0.150260538328439\n",
      "Validation loss, 0.20400546491146088\n",
      "Epoch: 55\n",
      "Training loss, 0.14993615448474884\n",
      "Validation loss, 0.19315674901008606\n",
      "Epoch: 56\n",
      "Training loss, 0.15207776706665754\n",
      "Validation loss, 0.13599558174610138\n",
      "Epoch: 57\n",
      "Training loss, 0.14749995665624738\n",
      "Validation loss, 0.13171657919883728\n",
      "Epoch: 58\n",
      "Training loss, 0.1483081984333694\n",
      "Validation loss, 0.13218596577644348\n",
      "Epoch: 59\n",
      "Training loss, 0.14259051531553268\n",
      "Validation loss, 0.1627574861049652\n",
      "Epoch: 60\n",
      "Training loss, 0.1425522668287158\n",
      "Validation loss, 0.17407457530498505\n",
      "Epoch: 61\n",
      "Training loss, 0.14880172070115805\n",
      "Validation loss, 0.14126567542552948\n",
      "Epoch: 62\n",
      "Training loss, 0.14310889970511198\n",
      "Validation loss, 0.20660445094108582\n",
      "Epoch: 63\n",
      "Training loss, 0.13702239096164703\n",
      "Validation loss, 0.1879013329744339\n",
      "Epoch: 64\n",
      "Training loss, 0.14474690426141024\n",
      "Validation loss, 0.10735628008842468\n",
      "Epoch: 65\n",
      "Training loss, 0.13553749723359942\n",
      "Validation loss, 0.12959139049053192\n",
      "Epoch: 66\n",
      "Training loss, 0.13063949206843972\n",
      "Validation loss, 0.10585138201713562\n",
      "Epoch: 67\n",
      "Training loss, 0.13002461940050125\n",
      "Validation loss, 0.20024371147155762\n",
      "Epoch: 68\n",
      "Training loss, 0.13765244279056787\n",
      "Validation loss, 0.14607498049736023\n",
      "Epoch: 69\n",
      "Training loss, 0.1281116222962737\n",
      "Validation loss, 0.12614749372005463\n",
      "Epoch: 70\n",
      "Training loss, 0.12590921856462955\n",
      "Validation loss, 0.1721101552248001\n",
      "Epoch: 71\n",
      "Training loss, 0.13122776430100203\n",
      "Validation loss, 0.11154813319444656\n",
      "Epoch: 72\n",
      "Training loss, 0.12646297318860888\n",
      "Validation loss, 0.09297680854797363\n",
      "Epoch: 73\n",
      "Training loss, 0.126272717025131\n",
      "Validation loss, 0.18623994290828705\n",
      "Epoch: 74\n",
      "Training loss, 0.12251233356073499\n",
      "Validation loss, 0.07491692155599594\n",
      "Epoch: 75\n",
      "Training loss, 0.12406342942267656\n",
      "Validation loss, 0.10417380928993225\n",
      "Epoch: 76\n",
      "Training loss, 0.11623283987864852\n",
      "Validation loss, 0.09120751172304153\n",
      "Epoch: 77\n",
      "Training loss, 0.12264501117169857\n",
      "Validation loss, 0.13194161653518677\n",
      "Epoch: 78\n",
      "Training loss, 0.1166785228997469\n",
      "Validation loss, 0.17535415291786194\n",
      "Epoch: 79\n",
      "Training loss, 0.11706061149016023\n",
      "Validation loss, 0.07578306645154953\n",
      "Epoch: 80\n",
      "Training loss, 0.11759294522926211\n",
      "Validation loss, 0.10665789246559143\n",
      "Epoch: 81\n",
      "Training loss, 0.12110615288838744\n",
      "Validation loss, 0.10061963647603989\n",
      "Epoch: 82\n",
      "Training loss, 0.11521302163600922\n",
      "Validation loss, 0.13070864975452423\n",
      "Epoch: 83\n",
      "Training loss, 0.11829924443736672\n",
      "Validation loss, 0.12889014184474945\n",
      "Epoch: 84\n",
      "Training loss, 0.10926049901172519\n",
      "Validation loss, 0.10945920646190643\n",
      "Epoch: 85\n",
      "Training loss, 0.11206033686175942\n",
      "Validation loss, 0.14608123898506165\n",
      "Epoch: 86\n",
      "Training loss, 0.1089219874702394\n",
      "Validation loss, 0.14333078265190125\n",
      "Epoch: 87\n",
      "Training loss, 0.11019958695396781\n",
      "Validation loss, 0.2356419712305069\n",
      "Epoch: 88\n",
      "Training loss, 0.11280949413776398\n",
      "Validation loss, 0.12068743258714676\n",
      "Epoch: 89\n",
      "Training loss, 0.10555570712313056\n",
      "Validation loss, 0.1495642364025116\n",
      "Epoch: 90\n",
      "Training loss, 0.10838882206007838\n",
      "Validation loss, 0.1682244837284088\n",
      "Epoch: 91\n",
      "Training loss, 0.10794805036857724\n",
      "Validation loss, 0.07999871671199799\n",
      "Epoch: 92\n",
      "Training loss, 0.1061415676958859\n",
      "Validation loss, 0.1570862978696823\n",
      "Epoch: 93\n",
      "Training loss, 0.1042076712474227\n",
      "Validation loss, 0.1737447828054428\n",
      "Epoch: 94\n",
      "Training loss, 0.09848579345270991\n",
      "Validation loss, 0.09934049099683762\n",
      "Epoch: 95\n",
      "Training loss, 0.10014713415876031\n",
      "Validation loss, 0.12412737309932709\n",
      "Epoch: 96\n",
      "Training loss, 0.0976119926199317\n",
      "Validation loss, 0.11563285440206528\n",
      "Epoch: 97\n",
      "Training loss, 0.1039068759419024\n",
      "Validation loss, 0.08195888251066208\n",
      "Epoch: 98\n",
      "Training loss, 0.09505660692229867\n",
      "Validation loss, 0.09576020389795303\n",
      "Epoch: 99\n",
      "Training loss, 0.09349405625835061\n",
      "Validation loss, 0.04283668473362923\n",
      "Epoch: 100\n",
      "Training loss, 0.09348626714199781\n",
      "Validation loss, 0.13430921733379364\n",
      "Epoch: 101\n",
      "Training loss, 0.0886645158752799\n",
      "Validation loss, 0.09665747731924057\n",
      "Epoch: 102\n",
      "Training loss, 0.09191965591162443\n",
      "Validation loss, 0.12421800941228867\n",
      "Epoch: 103\n",
      "Training loss, 0.09295766800642014\n",
      "Validation loss, 0.11530333012342453\n",
      "Epoch: 104\n",
      "Training loss, 0.09360058372840285\n",
      "Validation loss, 0.11297263950109482\n",
      "Epoch: 105\n",
      "Training loss, 0.09024957194924355\n",
      "Validation loss, 0.05528898909687996\n",
      "Epoch: 106\n",
      "Training loss, 0.09004246536642313\n",
      "Validation loss, 0.054535530507564545\n",
      "Epoch: 107\n",
      "Training loss, 0.08368782140314579\n",
      "Validation loss, 0.12924247980117798\n",
      "Epoch: 108\n",
      "Training loss, 0.08881345903500915\n",
      "Validation loss, 0.04335857927799225\n",
      "Epoch: 109\n",
      "Training loss, 0.08434656588360667\n",
      "Validation loss, 0.1582058221101761\n",
      "Epoch: 110\n",
      "Training loss, 0.0855401111766696\n",
      "Validation loss, 0.07506835460662842\n",
      "Epoch: 111\n",
      "Training loss, 0.08094348758459091\n",
      "Validation loss, 0.07701631635427475\n",
      "Epoch: 112\n",
      "Training loss, 0.08168699406087399\n",
      "Validation loss, 0.12830567359924316\n",
      "Epoch: 113\n",
      "Training loss, 0.08676617033779621\n",
      "Validation loss, 0.06414520740509033\n",
      "Epoch: 114\n",
      "Training loss, 0.08147826697677374\n",
      "Validation loss, 0.07980357110500336\n",
      "Epoch: 115\n",
      "Training loss, 0.08217470860108733\n",
      "Validation loss, 0.07322244346141815\n",
      "Epoch: 116\n",
      "Training loss, 0.08192121237516403\n",
      "Validation loss, 0.11617078632116318\n",
      "Epoch: 117\n",
      "Training loss, 0.07836109912022948\n",
      "Validation loss, 0.06669311970472336\n",
      "Epoch: 118\n",
      "Training loss, 0.07895068172365427\n",
      "Validation loss, 0.04176539182662964\n",
      "Epoch: 119\n",
      "Training loss, 0.07648599473759532\n",
      "Validation loss, 0.0669710785150528\n",
      "Epoch: 120\n",
      "Training loss, 0.07843302492983639\n",
      "Validation loss, 0.07550008594989777\n",
      "Epoch: 121\n",
      "Training loss, 0.07623244705609977\n",
      "Validation loss, 0.0806364044547081\n",
      "Epoch: 122\n",
      "Training loss, 0.0779198738746345\n",
      "Validation loss, 0.07160479575395584\n",
      "Epoch: 123\n",
      "Training loss, 0.07469994528219104\n",
      "Validation loss, 0.10069582611322403\n",
      "Epoch: 124\n",
      "Training loss, 0.07540304097346961\n",
      "Validation loss, 0.0382731556892395\n",
      "Epoch: 125\n",
      "Training loss, 0.07547992328181863\n",
      "Validation loss, 0.07468683272600174\n",
      "Epoch: 126\n",
      "Training loss, 0.06794646312482655\n",
      "Validation loss, 0.05611872673034668\n",
      "Epoch: 127\n",
      "Training loss, 0.06951601384207606\n",
      "Validation loss, 0.11943189054727554\n",
      "Epoch: 128\n",
      "Training loss, 0.07127824611961842\n",
      "Validation loss, 0.07282506674528122\n",
      "Epoch: 129\n",
      "Training loss, 0.07170882145874202\n",
      "Validation loss, 0.08600161969661713\n",
      "Epoch: 130\n",
      "Training loss, 0.07067350507713854\n",
      "Validation loss, 0.07505635917186737\n",
      "Epoch: 131\n",
      "Training loss, 0.06997466809116304\n",
      "Validation loss, 0.03658313304185867\n",
      "Epoch: 132\n",
      "Training loss, 0.07057854533195496\n",
      "Validation loss, 0.021207822486758232\n",
      "Epoch: 133\n",
      "Training loss, 0.06651922361925244\n",
      "Validation loss, 0.02229349873960018\n",
      "Epoch: 134\n",
      "Training loss, 0.06781524792313576\n",
      "Validation loss, 0.07718780636787415\n",
      "Epoch: 135\n",
      "Training loss, 0.07059220899827778\n",
      "Validation loss, 0.03503095731139183\n",
      "Epoch: 136\n",
      "Training loss, 0.07157380902208388\n",
      "Validation loss, 0.026215501129627228\n",
      "Epoch: 137\n",
      "Training loss, 0.06703004776500165\n",
      "Validation loss, 0.08924398571252823\n",
      "Epoch: 138\n",
      "Training loss, 0.07027876889333129\n",
      "Validation loss, 0.05882955342531204\n",
      "Epoch: 139\n",
      "Training loss, 0.06487653800286353\n",
      "Validation loss, 0.056418247520923615\n",
      "Epoch: 140\n",
      "Training loss, 0.0688593762461096\n",
      "Validation loss, 0.08678475767374039\n",
      "Epoch: 141\n",
      "Training loss, 0.06442764890380204\n",
      "Validation loss, 0.06228802725672722\n",
      "Epoch: 142\n",
      "Training loss, 0.06398293212987483\n",
      "Validation loss, 0.11413319408893585\n",
      "Epoch: 143\n",
      "Training loss, 0.06718483450822532\n",
      "Validation loss, 0.12883643805980682\n",
      "Epoch: 144\n",
      "Training loss, 0.06407984858378768\n",
      "Validation loss, 0.05783074349164963\n",
      "Epoch: 145\n",
      "Training loss, 0.06439539953134954\n",
      "Validation loss, 0.018166247755289078\n",
      "Epoch: 146\n",
      "Training loss, 0.06511061917990446\n",
      "Validation loss, 0.09488531202077866\n",
      "Epoch: 147\n",
      "Training loss, 0.06293050129897892\n",
      "Validation loss, 0.06540105491876602\n",
      "Epoch: 148\n",
      "Training loss, 0.062399165239185095\n",
      "Validation loss, 0.044117022305727005\n",
      "Epoch: 149\n",
      "Training loss, 0.06321472767740488\n",
      "Validation loss, 0.12048745155334473\n",
      "Epoch: 150\n",
      "Training loss, 0.06250147987157106\n",
      "Validation loss, 0.10703224688768387\n",
      "Epoch: 151\n",
      "Training loss, 0.06427113129757345\n",
      "Validation loss, 0.07585646212100983\n",
      "Epoch: 152\n",
      "Training loss, 0.061445703729987144\n",
      "Validation loss, 0.05081517994403839\n",
      "Epoch: 153\n",
      "Training loss, 0.06235644733533263\n",
      "Validation loss, 0.08204425871372223\n",
      "Epoch: 154\n",
      "Training loss, 0.0634473655372858\n",
      "Validation loss, 0.021095173433423042\n",
      "Epoch: 155\n",
      "Training loss, 0.06027851835824549\n",
      "Validation loss, 0.05497725307941437\n",
      "Epoch: 156\n",
      "Training loss, 0.06293850112706423\n",
      "Validation loss, 0.10226903855800629\n",
      "Epoch: 157\n",
      "Training loss, 0.06476291199214756\n",
      "Validation loss, 0.08823499828577042\n",
      "Epoch: 158\n",
      "Training loss, 0.06281108176335692\n",
      "Validation loss, 0.04399872571229935\n",
      "Epoch: 159\n",
      "Training loss, 0.06018630019389093\n",
      "Validation loss, 0.0657106265425682\n",
      "Epoch: 160\n",
      "Training loss, 0.06192339048720896\n",
      "Validation loss, 0.049402814358472824\n",
      "Epoch: 161\n",
      "Training loss, 0.05798018421046436\n",
      "Validation loss, 0.07116654515266418\n",
      "Epoch: 162\n",
      "Training loss, 0.0628095914144069\n",
      "Validation loss, 0.0903761014342308\n",
      "Epoch: 163\n",
      "Training loss, 0.06132391910068691\n",
      "Validation loss, 0.12118721753358841\n",
      "Epoch: 164\n",
      "Training loss, 0.06220823689363897\n",
      "Validation loss, 0.04046422988176346\n",
      "Epoch: 165\n",
      "Training loss, 0.05917214183136821\n",
      "Validation loss, 0.10036631673574448\n",
      "Epoch: 166\n",
      "Training loss, 0.058542954502627254\n",
      "Validation loss, 0.05575815960764885\n",
      "Epoch: 167\n",
      "Training loss, 0.05712054204195738\n",
      "Validation loss, 0.02497744932770729\n",
      "Epoch: 168\n",
      "Training loss, 0.057982363272458315\n",
      "Validation loss, 0.07114344090223312\n",
      "Epoch: 169\n",
      "Training loss, 0.05519192526116967\n",
      "Validation loss, 0.07280213385820389\n",
      "Epoch: 170\n",
      "Training loss, 0.05707069509662688\n",
      "Validation loss, 0.06237214058637619\n",
      "Epoch: 171\n",
      "Training loss, 0.05568507965654135\n",
      "Validation loss, 0.17935578525066376\n",
      "Epoch: 172\n",
      "Training loss, 0.05983129749074578\n",
      "Validation loss, 0.03340494632720947\n",
      "Epoch: 173\n",
      "Training loss, 0.054523627273738384\n",
      "Validation loss, 0.05483345687389374\n",
      "Epoch: 174\n",
      "Training loss, 0.05684306868351996\n",
      "Validation loss, 0.1022900715470314\n",
      "Epoch: 175\n",
      "Training loss, 0.053521705558523536\n",
      "Validation loss, 0.030612856149673462\n",
      "Epoch: 176\n",
      "Training loss, 0.052423525834456086\n",
      "Validation loss, 0.045020196586847305\n",
      "Epoch: 177\n",
      "Training loss, 0.05554544925689697\n",
      "Validation loss, 0.06917262822389603\n",
      "Epoch: 178\n",
      "Training loss, 0.05703802593052387\n",
      "Validation loss, 0.05459633842110634\n",
      "Epoch: 179\n",
      "Training loss, 0.05759542644955218\n",
      "Validation loss, 0.05206193029880524\n",
      "Epoch: 180\n",
      "Training loss, 0.054481058148667216\n",
      "Validation loss, 0.05704567953944206\n",
      "Epoch: 181\n",
      "Training loss, 0.05298470868729055\n",
      "Validation loss, 0.07814445346593857\n",
      "Epoch: 182\n",
      "Training loss, 0.056266969768330455\n",
      "Validation loss, 0.08478198945522308\n",
      "Epoch: 183\n",
      "Training loss, 0.05606265063397586\n",
      "Validation loss, 0.11209245771169662\n",
      "Epoch: 184\n",
      "Training loss, 0.048474572133272886\n",
      "Validation loss, 0.05661148950457573\n",
      "Epoch: 185\n",
      "Training loss, 0.05569537286646664\n",
      "Validation loss, 0.05679110437631607\n",
      "Epoch: 186\n",
      "Training loss, 0.051998666720464826\n",
      "Validation loss, 0.04043785110116005\n",
      "Epoch: 187\n",
      "Training loss, 0.051840671338140965\n",
      "Validation loss, 0.08044865727424622\n",
      "Epoch: 188\n",
      "Training loss, 0.050788423512130976\n",
      "Validation loss, 0.03063065931200981\n",
      "Epoch: 189\n",
      "Training loss, 0.05133321043103933\n",
      "Validation loss, 0.04656505957245827\n",
      "Epoch: 190\n",
      "Training loss, 0.04992655711248517\n",
      "Validation loss, 0.06327710300683975\n",
      "Epoch: 191\n",
      "Training loss, 0.05269343173131347\n",
      "Validation loss, 0.06129225715994835\n",
      "Epoch: 192\n",
      "Training loss, 0.052291660802438855\n",
      "Validation loss, 0.08595338463783264\n",
      "Epoch: 193\n",
      "Training loss, 0.0518644368276\n",
      "Validation loss, 0.0567934587597847\n",
      "Epoch: 194\n",
      "Training loss, 0.050503297708928585\n",
      "Validation loss, 0.03548596426844597\n",
      "Epoch: 195\n",
      "Training loss, 0.047494310420006514\n",
      "Validation loss, 0.031945254653692245\n",
      "Epoch: 196\n",
      "Training loss, 0.04942583432421088\n",
      "Validation loss, 0.016511758789420128\n",
      "Epoch: 197\n",
      "Training loss, 0.04842155403457582\n",
      "Validation loss, 0.07880840450525284\n",
      "Epoch: 198\n",
      "Training loss, 0.053040285129100084\n",
      "Validation loss, 0.07078330218791962\n",
      "Epoch: 199\n",
      "Training loss, 0.05330724664963782\n",
      "Validation loss, 0.006081588100641966\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "# model\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net.to(device)\n",
    "\n",
    "\n",
    "# margin value\n",
    "margin=1\n",
    "\n",
    "# criterion\n",
    "criterion = TripletLoss(margin,  RandomTripletSelector())\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_dataLoader=triplets_train_loader,\n",
    "                  validation_dataLoader=triplets_test_loader,\n",
    "                  epochs=200)\n",
    "\n",
    "# start training\n",
    "trainer.run_trainer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RqyseRnwjt4R"
   },
   "outputs": [],
   "source": [
    "## #TODO Modify this function such that you implement you character recognition algorithm here\n",
    "## The test code bellow will call this function with the following parameters \n",
    "## query - the query image (28, 28)\n",
    "## candidates - numpy array of candidate images, shape (5, 28, 28)\n",
    "## return - sorted array of the indexes of the images based on the similarty to the query image \n",
    "def test_model(query, candidates):\n",
    "    # TODO: dummy output that should be substituted by values produced by your solution\n",
    "    query = np.expand_dims(query, axis=1)\n",
    "    query = torch.tensor(query, dtype=torch.float).to(device)\n",
    "    out_query = model(query)\n",
    "    distances = []\n",
    "    for candidate in candidates:\n",
    "        candidate = np.expand_dims(np.expand_dims(candidate, axis=0), axis=0)\n",
    "        candidate = torch.tensor(candidate, dtype=torch.float).to(device)\n",
    "        out_candidate = model(candidate)\n",
    "        distance = (out_query - out_candidate).pow(2).sum(1)\n",
    "        distances.append(distance)\n",
    "    return np.argsort(distances) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMvs5VZuky4Q",
    "outputId": "c7a18bd0-450c-4934-c872-1ee9f5052343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 accuracy: 0.89\n",
      "top-3 accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## test top-1\n",
    "def test_top_1(query, candidates, query_true):\n",
    "  sorted_indexes = test_model(query, candidates)\n",
    "  return query_true == sorted_indexes[0]\n",
    "\n",
    "## test top-3\n",
    "def test_top_3(query, candidates, query_true):\n",
    "  sorted_indexes = test_model(query, candidates)\n",
    "  return np.isin(query_true, sorted_indexes[:3])\n",
    "\n",
    "top_1_res = np.array([test_top_1(a, b, c) for (a, b, c) in zip(queries, candidates_sets, queries_true)])\n",
    "top_3_res = np.array([test_top_3(a, b, c) for (a, b, c) in zip(queries, candidates_sets, queries_true)])\n",
    "\n",
    "top_1 = np.count_nonzero(top_1_res) / queries.shape[0]\n",
    "print(f\"top-1 accuracy: {top_1}\")\n",
    "\n",
    "top_3 = np.count_nonzero(top_3_res) / queries.shape[0]\n",
    "print(f\"top-3 accuracy: {top_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "workbook-task1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}