{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment1/workbook-task1.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Modify this cell to add your group name, group number and your names and student IDs\n",
    "\n",
    "Group: 99\n",
    "\n",
    "Authors: Yang Yang, Ying Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import xarray as xa\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training/testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_url(url):\n",
    "    \"\"\"\n",
    "    Loads a dataset from surfdrive. \n",
    "    \n",
    "    Input:\n",
    "    url: Download link of dataset \n",
    "    \n",
    "    Outputs:\n",
    "    x: Input features in numpy array format\n",
    "    y: Targets/labels in numpy array format\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    dataset = np.load(io.BytesIO(response.content)) \n",
    "    \n",
    "    x, y = np.split(dataset, [9], axis=2)\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "    \n",
    "# Downloading may take a while..\n",
    "train_x, train_y = load_dataset_from_url('https://surfdrive.surf.nl/files/index.php/s/gVrTFgSJ1rWl1IN/download')\n",
    "test_x, test_y = load_dataset_from_url('https://surfdrive.surf.nl/files/index.php/s/JR0WXbrzzTAmwEB/download')\n",
    "\n",
    "\n",
    "print(f\"train_x shape: {train_x.shape}\")\n",
    "print(f\"train_y shape: {train_y.shape}\\n\")\n",
    "\n",
    "print(f\"test_x shape: {test_x.shape}\")\n",
    "print(f\"test_y shape: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some of the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(x, y):\n",
    "    \"\"\"\n",
    "    Converts training/testing input features and corresponding labels into\n",
    "    a Pandas Dataframe format\n",
    "    \n",
    "    Inputs:\n",
    "    x: Input features (train or test) in numpy array format\n",
    "    y: Targets/labels (train or test) in numpy array format\n",
    "    \n",
    "    Output:\n",
    "    dataset_df: Train or test data, structered as a table with column names\n",
    "    \"\"\"\n",
    "    \n",
    "    numpy_data = np.concatenate([x,y], axis=2)\n",
    "    \n",
    "    dataset_df = xa.DataArray(numpy_data, \n",
    "                                     dims = ['N', 'frame', 'sensor'],\n",
    "                                     name='training_data')\\\n",
    "                                        .to_dataframe()\\\n",
    "                                        .unstack('sensor')['training_data']\\\n",
    "                                        .reset_index()\n",
    "\n",
    "    column_names = ['tot_acc_x', 'tot_acc_y', 'tot_acc_z', 'body_acc_x', 'body_acc_y',\n",
    "       'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'activity']\n",
    "    \n",
    "    dataset_df = dataset_df.rename(columns = dict(zip(list(dataset_df.columns[2:]), \n",
    "                                                      column_names)))\\\n",
    "                                         .astype({'activity':int})\n",
    "\n",
    "    return dataset_df\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_samples(N, dataset_df):\n",
    "    \"\"\"\n",
    "    Plots samples in test/train dataset\n",
    "    \n",
    "    Inputs \n",
    "    N: Number of samples that will be visualised. \n",
    "    dataset_df: Train or test data, structered as a table with column names. \n",
    "                This tabular structured data can be obtained with `to_df` function.\n",
    "    \"\"\"\n",
    "    \n",
    "    f, axes = plt.subplots(N, 4, figsize=(30, N*7))\n",
    "    axes = iter(axes)\n",
    "\n",
    "    for pid, df_pid in list(dataset_df.groupby('N'))[:N]:\n",
    "\n",
    "        ax_tot_acc, ax_body_acc, ax_body_gyro, ax_activity = tuple(next(axes))\n",
    "\n",
    "        df_pid.plot(x = 'frame', y=['tot_acc_x','tot_acc_y', 'tot_acc_z'], title=f'sample={pid}', ax=ax_tot_acc)\n",
    "        df_pid.plot(x = 'frame', y=['body_acc_x','body_acc_y', 'body_acc_z'], ax=ax_body_acc)\n",
    "        df_pid.plot(x = 'frame', y=['body_gyro_x','body_gyro_y', 'body_gyro_z'], ax=ax_body_gyro)\n",
    "        df_pid.plot(x = 'frame', y=['activity'], ax=ax_activity) \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "training_data_df = to_df(train_x, train_y)\n",
    "\n",
    "plot_training_samples(2, training_data_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Implement the solution to task 2 of assignment 1\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_dataLoader: torch.utils.data.Dataset,\n",
    "                 validation_dataLoader: torch.utils.data.Dataset ,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_dataLoader = training_dataLoader\n",
    "        self.validation_dataLoader = validation_dataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def run_trainer(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            self.model.train()  # train mode\n",
    "            train_losses=[]\n",
    "            for batch in self.training_dataLoader:\n",
    "                x, y = batch\n",
    "                input, target = x.to(device=self.device, dtype=torch.float), y.to(self.device)  # send to device (GPU or CPU)\n",
    "                self.optimizer.zero_grad()  # zerograd the parameters\n",
    "                out = self.model(input)  # one forward pass\n",
    "                loss = self.criterion(out, target)  # calculate loss\n",
    "\n",
    "                loss_value = loss.item()\n",
    "                train_losses.append(loss_value)\n",
    "\n",
    "                loss.backward()  # one backward pass\n",
    "                self.optimizer.step()  # update the parameters\n",
    "\n",
    "            self.model.eval()  # evaluation mode\n",
    "            val_losses = []  # accumulate the losses here\n",
    "\n",
    "            for batch in self.validation_dataLoader:\n",
    "                x, y = batch\n",
    "                input, target = x.to(device=self.device, dtype=torch.float), y.to(device=self.device, dtype=torch.float)  # send to device (GPU or CPU)\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(input)   # one forward pass\n",
    "                    loss = self.criterion(out, target) # calculate loss\n",
    "\n",
    "                    loss_value = loss.item()\n",
    "                    val_losses.append(loss_value)\n",
    "\n",
    "            print('Epoch:', epoch)\n",
    "            print('Training loss,', np.mean(train_losses))\n",
    "            print('Validation loss,', np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN \n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=9, out_channels=192, kernel_size=10, padding=1),\n",
    "            nn.BatchNorm1d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=2),\n",
    "            nn.Conv1d(in_channels=192, out_channels=96, kernel_size=10, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(96 * 250, 512)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 6)\n",
    "            #nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# TODO: 1Dconv + LSTM\n",
    "\n",
    "# TODO: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define customed dataset\n",
    "from torch.utils.data import Dataset\n",
    "class CustomAccelerometerDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        #'Initialization'\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #'Generates one sample of data'\n",
    "        # Select sample\n",
    "        return self.inputs[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split data into training set and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tf.keras.utils import to_categorical(y, num_classes=None, dtype=\"float32\")\n",
    "\n",
    "def split(inputs, targets):\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    targets = to_categorical(targets)\n",
    "    \n",
    "    # random seed\n",
    "    random_seed = 42\n",
    "\n",
    "    # split dataset into training set and validation set\n",
    "    train_size = 0.8  # 80:20 split\n",
    "    \n",
    "    # @parameter\n",
    "    # batch size for data loader\n",
    "    loader_batch_size = 2\n",
    "\n",
    "    inputs_train, inputs_valid = train_test_split(\n",
    "        inputs,\n",
    "        random_state=random_seed,\n",
    "        train_size=train_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    targets_train, targets_valid = train_test_split(\n",
    "        targets,\n",
    "        random_state=random_seed,\n",
    "        train_size=train_size,\n",
    "        shuffle=True)\n",
    "    \n",
    "    # dataset training\n",
    "    dataset_train = CustomAccelerometerDataset(inputs=inputs_train,\n",
    "                                              target=targets_train)\n",
    "\n",
    "    # dataset validation\n",
    "    dataset_valid = CustomAccelerometerDataset(inputs=inputs_valid,\n",
    "                                        targets=targets_valid)\n",
    "\n",
    "    # dataloader training\n",
    "    dataloader_training = DataLoader(dataset=dataset_train,\n",
    "                                     batch_size=loader_batch_size,\n",
    "                                     shuffle=True)\n",
    "\n",
    "    # dataloader validation\n",
    "    dataloader_validation = DataLoader(dataset=dataset_valid,\n",
    "                                       batch_size=loader_batch_size,\n",
    "                                       shuffle=True)  \n",
    "\n",
    "    return dataloader_training, dataloader_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    #@parameters:\n",
    "    epoch_num = 20\n",
    "    learning_rate = 0.001\n",
    "    momentum_val = 0.9\n",
    "    \n",
    "    # split training data and validatation data\n",
    "    dataloader_training, dataloader_validation = split(X, y)\n",
    "    \n",
    "    # device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device=torch.device('cpu')\n",
    "\n",
    "    # model\n",
    "    cnn = CNNClassifier()\n",
    "    model = cnn.to(device)\n",
    "\n",
    "    # criterion\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum_val)\n",
    "\n",
    "    # trainer\n",
    "    trainer = Trainer(model=model,\n",
    "                      device=device,\n",
    "                      criterion=criterion,\n",
    "                      optimizer=optimizer,\n",
    "                      training_dataLoader=dataloader_training,\n",
    "                      validation_dataLoader=dataloader_validation,\n",
    "                      epochs=epoch_num)\n",
    "\n",
    "    # start training\n",
    "    trainer.run_trainer()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    \n",
    "    # TODO: to be modified according to network structure\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x.to(device))  # send through model/network\n",
    "    out_softmax = torch.softmax(out, dim=1)  # perform softmax on outputs\n",
    "    \n",
    "    status = torch.argmax(out_softmax, dim=1)  # perform argmax to generate 1 channel\n",
    "    \n",
    "    print('predict', status)\n",
    "    return status\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "interval_size = 128\n",
    "step = 3\n",
    "\n",
    "#x = (n_sample, timestamp, channel), y = (n_sample, timestamp, 1)\n",
    "new_train_x = []\n",
    "new_train_y = []\n",
    "\n",
    "for sample, label in train_x, train_y:\n",
    "    for i in range(len(sample) - interval_size + 1, step):\n",
    "        interval_x = sample[i : i + interval_size]\n",
    "        interval_y = np.bincount(label[i : i + interval_size]).argmax()     \n",
    "        new_train_x.append(interval_x)\n",
    "        new_train_y.append(interval_y)\n",
    "    \n",
    "#train        \n",
    "model = train(new_train_x, new_train_y)\n",
    "\n",
    "# prediction on test data\n",
    "predicted = []\n",
    "for sample, label in test_x, test_y:\n",
    "    y_label = [[]] * len(sample)\n",
    "    for i in range(len(sample) - interval_size + 1, step):\n",
    "        interval_x = sample[i : i + interval_size]\n",
    "        interval_y = predict(interval_x, model)\n",
    "        \n",
    "        for j in range(i, i + interval_size):\n",
    "            y_label[j].append(interval_y)\n",
    "    \n",
    "    predict_y = [ [np.bincount(y_j).argmax()]  for y_j in y_label]\n",
    "    predicted.append(predict_y)\n",
    "    #compare test_y & predict_y\n",
    "\n",
    "    \n",
    "    \n",
    "# get multiclassification statistics    \n",
    "    \n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "y_test_array = [y_i for i in y for y in test_y]\n",
    "predicted_array = [y_i for i in y for y in predicted]\n",
    "\n",
    "precision, recall, fscore, support = score(y_test_array, predicted_array)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
